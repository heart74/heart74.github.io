{"meta":{"title":"心脏i","subtitle":null,"description":null,"author":"heart74","url":"https://heart74.github.io"},"pages":[{"title":"donate","date":"2020-02-08T05:14:00.000Z","updated":"2020-02-12T06:03:37.000Z","comments":false,"path":"donate/index.html","permalink":"https://heart74.github.io/donate/index.html","excerpt":"","text":"","keywords":"喜欢的哥哥姐姐可以资助一下哟，万分感激~"},{"title":"about","date":"2020-02-08T05:14:00.000Z","updated":"2020-02-12T06:03:04.000Z","comments":false,"path":"about/index.html","permalink":"https://heart74.github.io/about/index.html","excerpt":"","text":"[heart心脏i] 与&nbsp; heart&nbsp; （ 真（ま）白（しろ） ） 对话中... bot_ui_ini()","keywords":"关于"},{"title":"comment","date":"2020-02-08T05:14:00.000Z","updated":"2020-02-12T06:03:30.000Z","comments":true,"path":"comment/index.html","permalink":"https://heart74.github.io/comment/index.html","excerpt":"","text":"念两句诗 叙别梦、扬州一觉。 【宋代】吴文英《夜游宫·人去西楼雁杳》","keywords":"留言板"},{"title":"client","date":"2020-02-08T05:14:00.000Z","updated":"2020-02-12T06:03:26.000Z","comments":false,"path":"client/index.html","permalink":"https://heart74.github.io/client/index.html","excerpt":"","text":"直接下载 or 扫码下载：","keywords":"Android客户端"},{"title":"bangumi","date":"2020-02-08T05:14:00.000Z","updated":"2020-02-12T06:03:21.000Z","comments":false,"path":"bangumi/index.html","permalink":"https://heart74.github.io/bangumi/index.html","excerpt":"","text":"","keywords":null},{"title":"rss","date":"2020-02-08T05:14:00.000Z","updated":"2020-02-12T06:03:59.000Z","comments":true,"path":"rss/index.html","permalink":"https://heart74.github.io/rss/index.html","excerpt":"","text":""},{"title":"music","date":"2020-02-08T05:14:00.000Z","updated":"2020-02-22T09:53:36.000Z","comments":false,"path":"music/index.html","permalink":"https://heart74.github.io/music/index.html","excerpt":"","text":"","keywords":"喜欢的音乐"},{"title":"lab","date":"2020-02-08T05:14:00.000Z","updated":"2020-02-12T06:11:21.000Z","comments":false,"path":"lab/index.html","permalink":"https://heart74.github.io/lab/index.html","excerpt":"","text":"sakura主题···更多好看的插件还在研究中。。。","keywords":"Lab实验室"},{"title":"links","date":"2020-02-08T05:14:00.000Z","updated":"2020-02-12T06:03:49.000Z","comments":true,"path":"links/index.html","permalink":"https://heart74.github.io/links/index.html","excerpt":"","text":"","keywords":"友人帐"},{"title":"video","date":"2020-02-08T05:14:00.000Z","updated":"2020-02-12T06:04:27.000Z","comments":false,"path":"video/index.html","permalink":"https://heart74.github.io/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '放送时间: 2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 番组计划 这里将是永远的回忆 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"B站"},{"title":"tags","date":"2020-02-08T05:14:00.000Z","updated":"2020-02-12T06:04:04.000Z","comments":true,"path":"tags/index.html","permalink":"https://heart74.github.io/tags/index.html","excerpt":"","text":""},{"title":"theme-sakura","date":"2020-02-08T05:14:00.000Z","updated":"2020-02-12T06:09:56.000Z","comments":false,"path":"theme-sakura/index.html","permalink":"https://heart74.github.io/theme-sakura/index.html","excerpt":"","text":"Hexo主题Sakura由hojun大佬修改自WordPress主题Sakura，感谢原作者Mashiro和hojun","keywords":"Hexo 主题 Sakura 🌸"}],"posts":[{"title":"RL_Overview","slug":"RL-Overview","date":"2024-06-13T01:10:05.000Z","updated":"2024-06-18T06:47:06.306Z","comments":true,"path":"2024/06/13/RL-Overview/","link":"","permalink":"https://heart74.github.io/2024/06/13/RL-Overview/","excerpt":"","text":"-1 lilian RL0 简介 1 探索与利用 2 多臂老虎机 策略探索 e-greedy 问题：为啥e-greedy的增长率比贪心策略小 策略探索 积极初始化 策略探索 基于不确定性测度 策略探索 Thompson Sampling方法 根据概率选择动作 马尔可夫决策过程 基于动态规划的强化学习 基于模型的强化学习比如学习MDP模型和不学习MDP模型 蒙特卡洛方法 蒙特卡洛价值预测 模型无关控制方法 重要性采样 时序差分学习 SARSA","categories":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}],"tags":[],"keywords":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}]},{"title":"LHY_RL","slug":"LHY_RL","date":"2024-03-25T14:41:25.000Z","updated":"2024-06-17T09:26:30.497Z","comments":true,"path":"2024/03/25/LHY_RL/","link":"","permalink":"https://heart74.github.io/2024/03/25/LHY_RL/","excerpt":"","text":"引言 注意是梯度上升 蓝色部分除以一个p，直觉是为了归一化，采样时出现几率比较高的action，防止b出现次数多，那么继续调高b出现的机率 问题：不是很懂 Policy Gradient上一节就是Policy Gradient PPOOn-policy&amp;Off-policy on policy -&gt; off policy做法重要性采样，从另一个分布中采样 重要性采样$$p_\\theta$$和$$p_\\theta\\prime$$不能差太多，PPO就是解决这个问题 TRPO是PPO的前身，在J上加了对两个分布的约束，所以比较难算 KL距离不是参数上的距离，而是行为上的距离，把S带入actor可以得到action的分布，所以可以算这两个分布的距离 不直接算$$\\theta$$之间的分布是因为output和$$\\theta$$之间的变化不一定是一致的 PPO vs PPO2","categories":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}],"tags":[],"keywords":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}]},{"title":"VIT&&Swin","slug":"VIT_Swin","date":"2024-03-20T02:14:25.000Z","updated":"2024-05-18T08:00:57.126Z","comments":true,"path":"2024/03/20/VIT_Swin/","link":"","permalink":"https://heart74.github.io/2024/03/20/VIT_Swin/","excerpt":"","text":"相关博客：https://blog.csdn.net/m0_37605642/article/details/133821025 https://achilles-10.github.io/posts/tech/vit/ 正文问题PE位置编码，传统transformer和视觉领域有可学习的位置编码和固定的两种， 构造位置编码方法用整型值标记位置一种自然而然的想法是，给第一个token标记1，给第二个token标记2…，以此类推。这种方法产生了以下几个主要问题：（1）模型可能遇见比训练时所用的序列更长的序列。不利于模型的泛化。（2）模型的位置表示是无界的。随着序列长度的增加，位置值会越来越大。 用[0,1]范围标记位置了解决整型值带来的问题，可以考虑将位置值的范围限制在[0, 1]之内，其中，0表示第一个token，1表示最后一个token。比如有3个token，那么位置信息就表示成[0, 0.5, 1]；若有四个token，位置信息就表示成[0, 0.33, 0.69, 1]。但这样产生的问题是，当序列长度不同时，token间的相对距离是不一样的。例如在序列长度为3时，token间的相对距离为0.5；在序列长度为4时，token间的相对距离就变为0.33。 因此，我们需要这样一种位置表示方式，满足于：（1）它能用来表示一个token在序列中的绝对位置（2）在序列长度不同的情况下，不同序列中token的相对位置/距离也要保持一致（3）可以用来表示模型在训练过程中从来没有看到过的句子长度。 用二进制向量标记位置考虑到位置信息作用在input embedding上，因此比起用单一的值，更好的方案是用一个和input embedding维度一样的向量来表示位置。这时我们就很容易想到二进制编码。如下图，假设d_model = 3，那么我们的位置向量可以表示成 、 这下所有的值都是有界的（位于0，1之间），且transformer中的d_model本来就足够大，基本可以把我们要的每一个位置都编码出来了。 但是这种编码方式也存在问题：这样编码出来的位置向量，处在一个离散的空间中，不同位置间的变化是不连续的。假设d_model = 2，我们有4个位置需要编码，这四个位置向量可以表示成[0,0],[0,1],[1,0],[1,1]。我们把它的位置向量空间做出来： 如果我们能把离散空间（黑色的线）转换到连续空间（蓝色的线），那么我们就能解决位置距离不连续的问题。同时，我们不仅能用位置向量表示整型，我们还可以用位置向量来表示浮点型。 用周期函数（sin）来表示位置回想一下，现在我们需要一个有界又连续的函数，最简单的，正弦函数sin就可以满足这一点。我们可以考虑把位置向量当中的每一个元素都用一个sin函数来表示 sin(wt),w为角速度，w=2$$\\pi$$f，wt也能代表周期，首先ωt即为以ω为角速度，经过t秒，走了多少度，所以当频率越小的时候，角速度越小，对t的变化越不敏感 最初transfomer的位置编码：目前为止，我们的位置向量实现了如下功能：（1）每个token的向量唯一（每个sin函数的频率足够小）（2）位置向量的值是有界的，且位于连续空间中。模型在处理位置向量时更容易泛化，即更好处理长度和训练数据分布不一致的序列（sin函数本身的性质） 那现在我们对位置向量再提出一个要求，不同的位置向量是可以通过线性转换得到的。这样，我们不仅能表示一个token的绝对位置，还可以表示一个token的相对位置， 所以用了sin和cos pos是位置，i是维度，dmodel = 512，也使用了可学习的位置编码，发现和sin cos基本产生了相同的结果，选择正弦版本是因为它可以允许模型外推到比训练期间遇到的序列长度更长的序列。sin/cos函数的波长从2$$\\pi$$增长到2$$\\pi$$*10000 波长计算：波长λ=vT，其中v是波速。波长是一个周期内波前进的距离，所以频率越小，波长越长 VIT的位置编码Vision Transformer (ViT) 之所以使用可学习的位置编码，而不是像原始的 Transformer 那样使用固定的正弦和余弦函数（sin/cos）编码，主要是出于以下几个原因： 适应性和灵活性: 可学习的位置编码可以通过反向传播进行训练，自动调整以最优地适应训练数据的特征。这种适应性使得 ViT 更灵活地处理各种复杂的视觉任务，尤其是当输入图片的尺寸和内容差异较大时。 域差异: Transformer 最初是为处理自然语言任务设计的，其使用的固定正弦和余弦函数编码是基于序列数据（如文本）的性质设计的。相比之下，图像数据的结构和特性与文本数据存在显著差异。让位置编码可学习有助于模型更好地捕捉图像中的空间关系，而不是用于序列数据的预定义模式。 优化性能: 可学习的位置编码能够在训练过程中与模型的其他部分共同优化，从而更好地融合位置信息和视觉内容。这种协同优化有助于提高模型的整体性能，使其在各种视觉任务上更加准确。 灵活处理不同尺寸的输入: 固定的 sin/cos 编码在处理不同尺寸的图片时可能不那么灵活，而可学习的位置编码能够更自然地适应不同尺寸的输入，因为模型可以学习到如何根据实际输入尺寸调整位置信息的表示。 实验结果验证: 在ViT和其他类似使用可学习位置编码的视觉模型中，实验结果通常表明使用可学习位置编码能够带来更好的性能和效果。这些实验验证了可学习位置编码在视觉任务中的有效性。 综上所述了，虽然固定的 sin/cos 位置编码在原始的 Transformer 设计中有效，但在视觉任务中，可学习的位置编码因其更高的适应性、优化性能和对不同尺寸输入的灵活处理而成为更优选择。主要原因还是原始Transfomer的序列长度是可变的，且推理时的序列长度可能和训练的时候不一致 1D和2D三种嵌入插入方式： 对于一维和二维位置嵌入，我们尝试了三种不同的情况:(1)在它们模型的主干之后和将输入馈给Transformer编码器之前向输入添加位置嵌入(本文中所有其他实验的默认值);(2)学习并在每层开始的输入中添加位置嵌入;(3)在每层开始的输入中添加一个学习到的位置嵌入(层与层之间共享)。 Vit采用绝对位置编码的形式，也就是使用一个值来表征每个patch的绝对位置，并且基于可学习的方式，一般的定义方式为： pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim)) dropout = nn.Dropout(emb_dropout) x += self.pos_embedding[:, :(n + 1)] x = self.dropout(x) 我们可以看到，虽然没有位置嵌入的模型和有位置嵌入的模型在性能上有很大的差距，但是不同的位置信息编码方式之间几乎没有差别。我们推测，由于我们的Transformer编码器在补丁级输入上操作，而不是像素级输入，因此如何编码空间信息的差异不太重要。更准确地说，在补丁级输入中，空间维度比原始像素级输入小得多，例如，14 x14而不是224 x224，并且对于这些不同的位置编码策略来说，学习在这种分辨率下表示空间关系同样容易。 Swin transformer位置编码采用了相对位置编码的概念，考虑query和key的相对位置进行编码 绝对编码 （absoluate position）能提升性能，但是效果不如相对编码（relative position），仅仅是相对编码的效果等价于相对编码+绝对编码 我们观察到，与没有此偏差项或使用绝对位置嵌入的同类相比，有了显著的改进，如表4所示。进一步像VIT那样在输入中加入绝对位置嵌入会略微降低性能，因此在我们的实现中没有采用。预训练中习得的相对位置偏差也可用于初始化模型，通过双三次插值对不同窗口大小进行微调","categories":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}],"tags":[],"keywords":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}]},{"title":"ResNet&DenseNet","slug":"Resnet&Densenet&卷积神经网络","date":"2023-10-19T11:30:12.000Z","updated":"2024-05-17T03:00:25.155Z","comments":true,"path":"2023/10/19/Resnet&Densenet&卷积神经网络/","link":"","permalink":"https://heart74.github.io/2023/10/19/Resnet&Densenet&卷积神经网络/","excerpt":"","text":"卷积神经网络卷积层的输出尺寸、参数量和计算量 卷积核为什么用奇数不用偶数，如何设计卷积核原因1： 奇数相对于偶数，有中心点，对边沿、对线条更加敏感，可以更有效的提取边沿信息。也能获取到中心信息偶数也可以使用，但是效率比奇数低。在数以万计或亿计的计算过程中，每个卷积核差一点，累计的效率就会差很多。 原因2： 不好补pad 原因3： 卷积核设计：如何设计和选择卷积核的基本原则： 卷积核的大小 卷积核一般选择奇数大小，常见的如 1x1，3x3，5x5，7x7 等，这是为了让卷积操作有一个明确的中心。 较小的卷积核（如3x3）是卷积神经网络中最常用的，因为他们在少量的参数中可以捕获到感受野内的空间信息。此外，多层较小的卷积核堆叠起来可以达到和单层较大卷积核（如5x5，7x7）同样的感受野，但参数数量更少，计算效率更高。 卷积核的数量 卷积核的数量等于卷积层输出的通道数（depth）。卷积核的数量通常为 2 的幂次方，如 16, 32, 64, 128, 256等，因为这样的设计可以方便GPU进行运算。 卷积核的数量体现了模型的学习能力。在早期的卷积层，由于输入的特征（如图像）通常包含较少的抽象信息（如边缘，颜色，纹理等），所以卷积核的数量一般较少。但在深层的卷积层，模型需要学习更复杂的特征，此时一般要增加卷积核的数量来提高模型的学习能力。 选择卷积核数量的过程也是一个权衡过程，过少可能导致模型学习能力不足，无法捕获到所有的信息；过多可能导致模型过拟合，增加计算负担。 卷积核的初始化 通常，卷积核的权重是随机初始化的（如使用高斯分布），然后通过训练数据进行反向传播和梯度下降来进行学习和优化，这样每个卷积核最后可以学习到从输入数据中提取不同特征的能力。 卷积层的设计 卷积层设计还包括步长和填充的选择。步长（stride）决定了卷积核在输入上滑动的速度，较大的步长可以下采样输入，减少计算量；填充（padding）可以解决边界信息损失问题，保持输出的空间尺寸。 选择卷积核的大小和数量没有固定的公式，大多数情况下需要根据问题的具体情况和经验进行尝试和优化。模型的设计往往是一个试错的过程，需要尝试不同的组合并验证模型的性能。 AlexNet、 VGG NiN 网络中的网络LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。 AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。 或者，可以想象在这个过程的早期使用全连接层。然而，如果使用了全连接层，可能会完全放弃表征的空间结构。 网络中的网络（NiN）提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机（1*1卷积） GoogLeNet / Inception-v1在2014年的ImageNet图像识别挑战赛中，一个名叫GoogLeNet (Szegedy et al., 2015)的网络架构大放异彩。 GoogLeNet吸收了NiN中串联网络的思想，并在此基础上做了改进。 这篇论文的一个重点是解决了什么样大小的卷积核最合适的问题。 毕竟，以前流行的网络使用小到，大到的卷积核。 本文的一个观点是，有时使用不同大小的卷积核组合是有利的。 本节将介绍一个稍微简化的GoogLeNet版本：我们省略了一些为稳定训练而添加的特殊特性，现在有了更好的训练方法，这些特性不是必要的 那么为什么GoogLeNet这个网络如此有效呢？ 首先我们考虑一下滤波器（filter）的组合，它们可以用各种滤波器尺寸探索图像，这意味着不同大小的滤波器可以有效地识别不同范围的图像细节。 同时，我们可以为不同的滤波器分配不同数量的参数。 为了缓解单纯用池化层下采样带来的表达瓶颈问题，在原始 Inception 模块上修改，将每条支路最后一层的步长改为2，如下图。 ResNetBN ResNet 的提出基于这样一种现象：随着网络层数加深，训练误差和测试误差都会上升，这种现象称为网络退化（degeneration）。ResNet 使用跳层连接（shortcut connection）来解决这个问题，跳层连接有以下两点好处： 抑制梯度消失的现象，使网络在加深时性能不会下降； 由于 “近道” 的存在，若网络在层数加深退化时，可以通过控制 “近道” 和 “非近道” 的组合比例来退回到之前浅层时的状态，即 “近道” 具备自我关闭能力。 左正常块，右残差块 ResNet沿用了VGG完整的3x3卷积层设计。 残差块里首先有2个有相同输出通道数的卷积层。 每个卷积层后接一个批量规范化层和ReLU激活函数。 然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。 这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。 如果想改变通道数，就需要引入一个额外的卷积层来将输入变换成需要的形状后再做相加运算。 残差块的实现如下： import torch from torch import nn from torch.nn import functional as F from d2l import torch as d2l class Residual(nn.Module): #@save def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1): super().__init__() self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides) self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1) if use_1x1conv: self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides) else: self.conv3 = None self.bn1 = nn.BatchNorm2d(num_channels) self.bn2 = nn.BatchNorm2d(num_channels) def forward(self, X): Y = F.relu(self.bn1(self.conv1(X))) Y = self.bn2(self.conv2(Y)) if self.conv3: X = self.conv3(X) Y += X return F.relu(Y) 整体框架 DensenNet​","categories":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}],"tags":[],"keywords":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}]},{"title":"transformer","slug":"transformer","date":"2023-09-22T11:45:13.000Z","updated":"2024-05-18T08:17:44.356Z","comments":true,"path":"2023/09/22/transformer/","link":"","permalink":"https://heart74.github.io/2023/09/22/transformer/","excerpt":"","text":"传统的序列转录模型，基于复杂的循环或者卷积，主流模型，trans都给换成多头注意力了 模型架构 编码器编码器和解码器分别有6个layer，即N=6，一共12个layer，编码器和解码器各6个，一个层有两个子层，一个是多头注意力层，一个是MLP，对每一个子层都用了残差连接，最后使用了layer normalization. output of 子层 LayerNorm(x + Sublayer(x))。残差需要输入和输出一样大小，简单起见每一个层的输出维度都是512。和CNN MLP不一样，要么维度下减，channel增加。 layer norm 和 batch norm 图中行代表样本，每一列一个特征，左图是BN，右图是LN（二维图像） 图中左边蓝色线是batch norm，取一个特征，把他样本里所有元素都搞出来，归一化。layer norm（黄线）针对一个样本，所有特征，做归一化 seq是序列长度，一个样本是一个序列，类比图像里的（h*w = n）像素数，feature可以类比图像的通道数 batch norm在一个batch里，在某个feature得维度（样本方向），把均值变成0，方差变成1；还会学习$\\lambda和\\beta$，任意方差为某个值，均值为某个值 layer norm在样本的维度，特征方向，做归一化 解码器和编码器不一样的是，用了第三个子层（掩码多头注意力层），解码器做的是自回归（当前输入是过去时刻的输出），但是在注意力机制里，每次能看到完整的输入，所以要把t时刻以后的mask起来。 Attention机制注意力函数是将一个query和一些key-value映射成一个输出的一个函数，都是向量，output是value的加权和。输出的维度和value的维度一致。每一个value的权重，是value对应的key和查询的query的相似度计算出来的 如图所示，value的靠左边的权重（粗线条），由于query和左边Key更相似，所以更大，output是V的加权和 Scaled Dot-Product Attention 两个向量做内积，值越大，越相似。所以这里V的权重是Q和K的内积，算出来后除以向量的维度，做scale，再用softmax得到权重。实际中用矩阵进行运算。 如上图，Q矩阵相乘后，对n*m矩阵除以√dk，然后对每一行做softmax，结果和V相乘得到输出矩阵（最右边）输出矩阵的每一行（和q一样有n行）就是一个查询对应的值 一般有两种比较常见注意力机制，一种叫加性的注意力，可以处理query和key不等长（维度？），一种叫做dot product注意力，和本文不一样的是本文除以了√dk。用dot product是因为更加简单高效。 当dk不是很大的时候除不除都没关系，当dk比较大的时候，做点积，值会比较大或者比较小，值比较大的时候，同一行相对值的方差就更大，经过softmax容易有单个值变成1，其他值更加向两端靠拢，导致梯度比较小，transfomer dk比较大，所以除一下比较好 mask机制 maks的效果是，让我t时刻的query只看t时刻前面对应的key-value pair，计算出QK矩阵之后，直接将t时刻之后对应的值置为很大的负数，这样经过softmax就会变成0，从而实现忽略t时刻以后的k和v 多头注意力 与其做单个注意力，不如把q k v投影h次，做h个注意力，最后再投影回来，Linear的作用就是把向量投影到较低的维度 h = 8 用8个头，每次除以8，投影到64维 做多头的原因：因为注意力计算没有过程里没有可以学习的参数，让不同的Linear学习不同的特征，投影（线性层）的权重W是可以学的，可以学H个投影方法，来匹配不同的模式需要的一些相似函数，有点像有多个输出通道的感觉，一个卷积核和多个卷积核的区别 Position-wise 前馈神经网络有两层,W1的形状为(512,2048),W2的形状为(2048,512) x是query的输出，也就是512维，MLP把它投影到2048，又投影回去 左边transfomer，右边RNN，所以transformer用的是序列的全局信息 Positional Embeddingattention没有时序信息，因为输出是value加权和，权重是value和key之间的距离，和时序信息无关，词的顺序会变值不会变。RNN上一时刻输出到下一个时刻输入，本来就有位置信息。 用512向量表示位置数字，用周期不一样(不同频率)的sin和cos函数算出来的 self attention 时间复杂度 第四行是受限的self attention， 第一是层的类型 第二列是每层的计算复杂度， 第三列是顺序计算操作，意思是你下一步计算需要等前面多少步计算完成，越小说明并行程度越高， 最后一列是一个信息从一个数据点到另一个数据点需要走多远吗，越短越好 self-att:计算复杂度，n是序列长度，d是向量维度，query和key都是nd维，计算复杂度是O(n^2 xd), query的每一行和key的每一列乘以下求和，计算复杂度是d（d维向量），计算的结果矩阵是nxn维的，所以总的计算复杂度是dxnxn = O(n^2 xd)，还有一些别的矩阵运算，复杂度都是一样的 顺序计算操作：由于矩阵计算并行度比较高，可以看成是O(1) 路径长度：key-value和query 一次就能过来，所以是O(1) Recurrent:计算复杂度：序列长度为n， 顺序计算操作：由于要顺序计算，等前面的计算完成，所以要O(n) 路劲长度同上同理 Conv:待定，dxd分别是，输入通道数和输出通道数， 为什么卷积核kernel的形状是kxd？ Restricted SA:query只和最近的r个邻居做运算，所以是r . n. d， 一般都不用受限的SA###transformer时间复杂度主要在于Self Attention的时间复杂度 为什么要自用注意力做实验和其他 循环层卷积层比较，比较好，复杂度低 为什么除以dkQ*K的方差是dk，均值是0，等于进行标准化","categories":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}],"tags":[{"name":"学习","slug":"学习","permalink":"https://heart74.github.io/tags/学习/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}]},{"title":"RCNN系列&&Cascade_rcnn","slug":"cascade-rcnn","date":"2023-04-19T03:10:13.000Z","updated":"2024-04-11T01:52:14.462Z","comments":true,"path":"2023/04/19/cascade-rcnn/","link":"","permalink":"https://heart74.github.io/2023/04/19/cascade-rcnn/","excerpt":"","text":"Two StageStage1用Region Proposal Network (RPN)生成高质量的预选区域(RP)，Stage2用检测头以这些RP为输入，进行最终框框坐标的输出以及类别的判断。 代表算法：R-CNN、Fast R-CNN、Faster R-CNN和R-FCN等 RCNN(Region with CNN features)先用搜索算法（selective search）选出一些rp，将这些rp划分的图片rescale到固定的大小，然后输入预训练好的CNN获得features。最后用SVM对这些features作目标存在判断和分类。 Fast RCNN RCNN对每个region都进行CNN提取特征，由于region重叠部分较多，使CNN对相同区域提取多次，耗费时间。 并且RCNN提取region特征之前需要resize，破坏图中内容的结构，影响效果。 所以Fast R-CNN诞生了。 如图，Fast R-CNN将整张图片resize送入神经网络（RCNN是对每个region进行resize），在最后一层再加入候选框信息（这些候选框还是经过Selective Search提取，再将在原图中的位置映射到最后一层特征图上），这样CNN提取特征只需对整张图做一次提取即可。 使用ROI pooling layer对最后一层特征图中每个region的对应的区域进行pooling，产生固定大小的输出，再送进FC层。 FC层的输出两部分，使用了多任务损失函数(multi-task loss)。 ROI pooling layer 作用：对任意大小的输入产生固定的输出 损失函数 smooth的图像为： ##Faster RCNN Fast R-CNN与R-CNN一样需要预先Selective Search，该过程速度较慢。所以Faster R-CNN将region提取的过程融入进网络里。 提出了RPN(Region Proposal Network)，可以不费力地生成RP。 rp生成后，送入到Fast R-CNN结构中，计算每个proposal和标签框之间的iou，通过人为的设定一个IoU阈值（通常为0.5），把这些Proposals分为正样本（前景）和负样本（背景），并对这些正负样本采样，使得他们之间的比例尽量满足（1:3，二者总数量通常为128），之后这些proposals（128个）被送入到Roi Pooling，最后进行类别分类和box回归。 从图中可以看出，该网络结构大部分与Fast R-CNN相同，唯一不同的是最后一层卷积的输出feature map输入给了RPN网络来生成region位置，并使用生成的位置和最后一层feature map作为ROI pooling layer的输入。 RPN headRPN可以帮我们找出可能包含物体的那些区域 RPN使用固定大小的anchor，这些anchor将会均匀地放置在整个原始图像中。不同于原来我们要检测物体在哪里，我们现在利用anchor将问题转换为另外两部分： 某个框内是否含有物体 某个框是否框的准，如果框的不准我们要如何调整框 anchors如果直接学习物体边框(xmin,xmax,ymin,ymax) 是困难的： 边框数量不定，网络很难输出变长的数据 物体是不同大小有不同的宽高比，那训练一个效果很好的检测模型将会是非常复杂的 会存在一些无效的预测，比如xmax&lt;xmin时 所以RPN使用anchors的方法。RPN在feature map对应的原图位置上，以每个点为中心上使用9个anchors: 三种面积{128,256,512} 三种比例{1:1,1:2,2:1} 如下图所示 流程完整的图片输入CNN，得到完整的feature，根据这个总feature划分多个锚框(anchor box)。以一个feature像素点为中心，不同的长宽比和缩放比来生成该像素点的不同的锚框。每个像素点都有这一系列锚框。 得到锚框后，对每个框内的feature做一个前景和背景的二分类，用于判断目标是否位于锚框内。此外还需要将锚框调整到和bounding box相近的状态，可以用卷积完成这里两个个任务。一部分输出分类矩阵：rpn_cls_score，一部分输出调整后的边界框：rpn_bbox_pred。 接着RPN会根据这里两个值输出rpn_rois（Region of Interests）：表示可能有目标的框；和rpn_roi_probs：包含目标的可能性。这两者组成输出给rcnn的proposal，此时的回归和分类都比较粗糙，需要rcnn进一步精细化。 FPN(Feature Pyramid Networks)依赖CNN给出的最后一层feature能够很好地分类，但是对目标框的检测就没有那么友善。FPN提出对每一层的feature都去做分类和回归任务。因此每一层都有一个RPN头。 Cascade RCNN 对于Faster RCNN，IoU的设置对结果影响很大。如果IoU设置较小，那么预测框将会包含大量背景噪声信息预测，精度降低；如果IoU设置较大，那么符合标准的预测框会大量减少，容易过拟合。只有当设置的预测框IoU和rp的IoU接近才有比较好的效果。 Cascade RCNN提出级联多个rcnn头，从H1到H3逐步增加IoU的阈值，每次都重新对RP进行采样，好逐步学到多层次的信息，适应不同的分布。 区别RCNN直接用selective search选择Region Proposals，然后将这些RP对应的图片resize到固定大小，后输入预训练好的CNN获得features。最后用SVM对这些features作目标存在判断和分类。 缺点： 对每个region都进行CNN提取特征，由于region重叠部分较多，使用CNN对相同区域提取多次耗时 RCNN提取Region特征之前需要Resize，破坏图中的内容的结构，影响性能 Fast RCNN 先将正常图片输入预训练好的CNN提取特征，然后在最后一层再使用selective search选取一些Region Proposals候选框（将原图中的位置映射到特征图），CNN只需提取一次特征 然后通过一个ROIpooing，对特征图中每个region的对应的区域进行pooling，产生固定大小的输出，输入FC层进行分类和回归任务 FC层的输出两部分，使用了多任务损失函数(multi-task loss) 缺点： Fast R-CNN与R-CNN一样需要预先Selective Search，该过程速度较慢 Faster RCNN针对Fast RCNN的缺点，Faster R-CNN将region提取的过程融入进网络里，使用RPN进行提取rp，然后再输入分类头 RPN可以帮我们找出可能包含物体的哪些区域（类别置信度，和bbox坐标） RPN使用固定大小的anchor，这些anchor将会均匀地放置在整个原始图像中。不同于原来我们要检测物体在哪里，我们现在利用anchor将问题转换为另外两部分： 某个框内是否含有物体 某个框是否框的准，如果框的不准我们要如何调整框 流程完整的图片输入CNN，得到完整的feature，根据这个总feature划分多个锚框(anchor box)。以一个feature像素点为中心，不同的长宽比和缩放比来生成该像素点的不同的锚框。每个像素点都有这一系列锚框。 得到锚框后，对每个框内的feature做一个前景和背景的二分类，用于判断目标是否位于锚框内。此外还需要将锚框调整到和bounding box相近的状态，可以用卷积完成这里两个个任务。一部分输出分类矩阵：rpn_cls_score，一部分输出调整后的边界框：rpn_bbox_pred。 接着RPN会根据这里两个值输出rpn_rois（Region of Interests）：表示可能有目标的框；和rpn_roi_probs：包含目标的可能性。这两者组成输出给rcnn的proposal，此时的回归和分类都比较粗糙，需要rcnn进一步精细化。 依赖CNN给出的最后一层feature能够很好地分类，但是对目标框的检测就没有那么友善。FPN(特征金字塔)提出对每一层的feature都去做分类和回归任务。因此每一层都有一个RPN头。 缺点： 对于Faster RCNN，IoU的设置对结果影响很大。如果IoU设置较小，那么预测框将会包含大量背景噪声信息预测，精度降低；如果IoU设置较大，那么符合标准的预测框会大量减少，容易过拟合。只有当设置的预测框IoU和rp的IoU接近才有比较好的效果。 没看懂： Cascade RCNNCascade RCNN提出级联多个rcnn头，从H1到H3逐步增加IoU的阈值，每次都重新对RP进行采样，好逐步学到多层次的信息，适应不同的分布。 问题一阶段和两阶段的区别1、YOLO是一阶段的方法，速度比RCNN系列更快。 2、YOLO在CNN中计算了图像中的全部特征，而RCNN在分类时系列只看到了局部特征。即YOLO使用全图作为 Context 信息，背景错误（把背景错认为物体）比较少 YOLOv1对小目标检测差，每个cell只测两个框，两个框都是一个类别的，当小目标聚集在一起，效果很差 定位不准确 One-Stage检测算法，没有selective se arch产生region proposal的阶段，直接产生物体的类别概率和位置坐标，经过单次检测即可直接获得最终的检测结果。相比Two-Stage有更快的速度。代表网络有YOLO v1/v2/v3/9000,SSD,Retina-Net. （two-stage算法中的roi pooling会对目标做resize, 小目标的特征被放大，其特征轮廓也更为清晰，因此检测也更为准确） Two-Stage检测算法将检测问题划分成两个阶段，首先是获取region proposal进行位置精修和分类阶段。相比于One-Stage,精度高，漏检率也低，但是速度较慢，代表网络有Fast rcnn，Faster rcnn，mask rcnn等。 Two-Stage和One-Stage的异同（回答的是Two-Stage先对前景背景做了筛选，再进行回归，回归效果比较好，准度高但是相比较慢，One-Stage是直接对特征上的点进行直接回归，优点是速度快，因为用了多层特征图出框可能小目标效果比较好一点（个人看法），缺点是因为正负样本失衡导致效果较差，要结合难例挖掘。） one stage在哪些具体方面检测精度不高（ROI+default box的深层理解）（one-stage算法对小目标检测效果较差，如果所有的anchor都没有覆盖到这个目标，那么这个目标就会漏检。） Faster rcnn的两阶段训练和end-to-end训练的不一样 （回答的是就是把RPN和二阶段拆开训，然后追问RPN在ENDTOEND中怎么回传，答TOTALLoss中有一阶段和二阶段的LOSS，只是回传影响的部分不一样。） 目标检测的发展历程，从传统到深度（传统部分回答的算子结合分类器分类，简单说了一下缺陷，深度部分说了RCNN,FAST,FASTER,SSD,YOLO,FPN,MASK RCNN,Cascade RCNN，都简单的介绍了一下） 传统目标检测：主线：区域选择-&gt;特征提取-&gt;分类器 传统的做目标检测的算法基本流程如下：1. 使用不同尺度的滑动窗口选定图像的某一区域为候选区域；2. 从对应的候选区域提取如Harr HOG LBP LTP等一类或者多类特征；3. 使用Adaboost SVM 等分类算法对对应的候选区域进行分类，判断是否属于待检测的目标。 缺点：1）基于滑动窗口的区域选择策略没有针对性，时间复杂度高，窗口冗余2）手工设计的特征对于多样性的变化没有很好的鲁棒性 多层的Feature预测的结果如何融合涉及到使用FPN的网络，答案是不用融合，每个尺度都可以输入框，最后可以用nms之类的方法进行抑制 selective search ROI pooling和SPP的关系SPP叫做空间金字塔池化，对任意大小的输入产生固定的输出，解决多尺度问题 yolov4的介绍里有SPP RPN实现细节3x3 卷积 + 两个1x1卷积 分别是前景后景类别预测，以及proposals框的坐标预测","categories":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}],"tags":[],"keywords":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}]},{"title":"yolov8","slug":"yolov8","date":"2023-03-19T02:12:49.000Z","updated":"2024-05-12T14:55:32.640Z","comments":true,"path":"2023/03/19/yolov8/","link":"","permalink":"https://heart74.github.io/2023/03/19/yolov8/","excerpt":"","text":"v8网络结构 特性和改动 上图右下角是 BCE+CIOU+DFL 提供了一个全新的SOTA模型，包括P5640和P61280分辨率的目标检测网络和基于YOLACT的实例分割模型。和YOLOv5一样，基于缩放系数也提供了N/S/M/L/X尺度的不同大小模型，用于满足不同场景需求 将CSP结构换成了梯度流更丰富的C2f结构，并对不同模型调整了不同的通道数，对模型结构微调(由于C2f结构有着更多的残差连接，所以其有着更丰富的梯度流。) Head部分相比yolov5改动较大，换成了解耦头结构，将分类和检测头分离，同时也从Anchor-Based换成了Anchor Free Loss计算方面采用了TaskAlignedAssignedAssigner正样本分配策略，并引入了Distribution Focal Loss 训练的数据增强部分，引入了最后10个epoch关闭Mosiac增强的操作，可以有效地提升精度 骨干网络和Neck的具体变化 DFL介绍： 效果：良心技术，别问，问就是无cost涨点 一句话总结：基于任意one-stage 检测器上，调整框本身与框质量估计的表示，同时用泛化版本的GFocal Loss训练该改进的表示，无cost涨点（一般1个点出头）AP https://zhuanlan.zhihu.com/p/147691786 Loss计算","categories":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}],"tags":[],"keywords":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}]},{"title":"yolov5","slug":"yolov5","date":"2023-01-10T11:50:00.000Z","updated":"2024-04-11T02:12:43.208Z","comments":true,"path":"2023/01/10/yolov5/","link":"","permalink":"https://heart74.github.io/2023/01/10/yolov5/","excerpt":"","text":"框架图Focus模块已经被stem layer替换，作用都是下采样 相较于v3，把SPP换成了SPPF，由并行到串行，效率更高，计算结果一样 YOLOv4 和 YOLOv5 都使用 CSPDarknet作为BackBone，从输入图像中提取丰富的特征信息。CSPNet叫做Cross Stage Partial Network，跨阶段局部网络。其解决了其他大型卷积网络结构中的重复梯度问题，减少模型参数和FLOPS。这对YOLO 有重要的意义，即保证了推理速度和准确率，又减小了模型尺寸。 数据增强MosiaicCopy Paste仿射变换MixUpAlbumentations滤波、直方图均衡化、改变图片质量 HSVFLIP训练策略 损失 平衡不同尺度损失 消除Grid敏感度 匹配正样本 问题obj损失的CIOU和loc损失的CIOU什么区别关于损失可以看yolov3的介绍，obj损失是置信度损失，用IOUorCIOU计算，用BCE算的，而loc是回归损失（预测框的坐标），损失本身就是CIOU，回归损失只计算正样本的 Stem layer是什么 Stemblock 结构将输出的尺寸缩减为输入的 1/4，多用于轻量化网络，完成下采样操作，可以用于 YOLOv5 网络模型中原始的卷积下采样操作，减少参数量。 Stemblock 结构是 PeleeNet 中用于下采样的方法。该模块能够确保较强的特征表达能力且能够减少大量的参数 由上图可以看到原始 Stemblock 结构最终将输出的尺寸缩减为输入的 1/4，参照 Stemblock 结构并对其进行修改使其只完成一次下采样即可，主要目的是替代 YOLOv5 网络模型中原始的卷积下采样操作，减少参数量。因此最终使用的修改后的 stemblock 结构如下图所示 最终yolov5使用的是，缩减为输出的1/2","categories":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}],"tags":[],"keywords":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}]},{"title":"yolov4","slug":"yolov4","date":"2022-10-24T01:34:15.000Z","updated":"2024-04-11T04:22:39.840Z","comments":true,"path":"2022/10/24/yolov4/","link":"","permalink":"https://heart74.github.io/2022/10/24/yolov4/","excerpt":"","text":"v4 相较v3，引入了CSP层，可以增强CNN的学习能力，移除计算瓶颈，最后能减少内存消耗，Darknet同样是53 SPP PAN Backbone 消除Grid敏感程度当gt 在gird的边界上时，要求sigmiod（x）=0，但是当x负无穷才会趋近于0，不可取 优化后，取得的值域变成[-0.5，1.5] 优化Anchor","categories":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}],"tags":[],"keywords":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}]},{"title":"yolov1-v3","slug":"yolov1-v3","date":"2022-10-18T11:55:23.000Z","updated":"2024-05-17T01:05:58.983Z","comments":true,"path":"2022/10/18/yolov1-v3/","link":"","permalink":"https://heart74.github.io/2022/10/18/yolov1-v3/","excerpt":"","text":"v1 损失函数 注意分类损失没有用交叉熵，用的误差平方和 YOLOv1对小目标检测差，每个cell只测两个框，两个框都是一个类别的，当小目标聚集在一起，效果很差 定位不准确 yolov2 相较于v1的提升 BN每个卷积后面添加了BN层，减少了一系列正则化处理， 更高分辨率的分类器一开始yolov1用的224x224预训练，yolov2采用更大的分类尺寸448x448 使用基于Anchor的边界框预测简化了问题，更加容易学习和收敛，召回率增加 Anchor聚类使网络更容易学习 直接位置预测对预测的相对Anchor的坐标偏移量进行了限制 细粒度特征关注小目标，类似resnet的pass through方法，融合高层和低层体征 多尺度训练每10个batch，随机选择图片尺寸，{320,352,…..608}的尺寸 BackBoneDarknet-19 19个卷积层 yolov3backbone 没有通过最大池化，用卷积层步数为2进行下采样，卷积核个数少，参数少，速度快 在小特征图（1）预测大目标，大特征图（3）预测小目标 网络预测类似FPN 目标边界框的预测（和v2相同） 正负样本匹配 对每个GT，分配一个Anchor作为正样本 损失 CIOU IOU loss GIOU 缺点：某些情况下会退化成IOU，但是问题不大？ u是两个框并起来的面积，Ac是两个矩形的最小外接矩形的面积，也就是蓝色框的面积，那么当两个框不相交的时候，Ac-U就会比较大，不会导致GIou为0 DIOU LOSS IoU值不能反映两个框是如何相交的，在中心明显更好 CIou Loss Focal Loss 增加难分样本权重，减少易分样本权重，容易受到噪声干扰 Mosaic SPP 问题anchor到底是什么？先验候选框？往anchor上去学习，降低学习难度，加快训练速度，学习目标是anchor相对于GT的偏移 目标检测算法 assigner的作用在目标检测算法中，assigner 的作用是在训练阶段将预测框（也称为候选框或anchors）（预测的结果是anchors相对于GT的偏移）分配给正确的类别标签和真实框（ground truth boxes）。这是训练检测模型过程中的一个关键步骤，因为这确保了模型能够从适当的例子中学习，从而提高目标检测的准确率。 目标检测模型依赖于大量的预测框来覆盖图片中可能出现的物体位置。这些预测框是在训练和推断阶段前生成的，通常有很多个，并且覆盖了不同的尺寸和纵横比。目标是识别出哪些预测框与真实存在的物体匹配，以及这些物体属于哪个类别。 assigner 的任务可以总结为以下几点： 正负样本分配：区分这些预测框是作为正样本（即与真实框有较高重叠度的预测框）还是负样本（与真实框重叠度较低或没有重叠的预测框）。重叠度一般通过计算预测框和真实框之间的交并比（IoU, Intersection over Union）来确定。具体的IoU阈值取决于具体的算法和设置。 多任务学习分配：一旦确定了正样本，assigner 还需要将每个正样本预测框分配给对应的真实框。这是为了训练模型去学习如何调整这些预测框使其更接近真实框（即框回归），以及如何正确识别物体的类别。 控制样本平衡：在训练目标检测模型时，通常会存在正负样本不平衡的情况。assigner 也可能会采取策略，如过采样正样本或负样本，或者使用其他技术来确保模型不会偏向于最频繁的类别。 不同的目标检测算法可能使用不同的assigner策略。例如，Faster R-CNN 使用 Region Proposal Network (RPN) 来生成候选区域，并使用预设的IoU阈值来分配正负样本；而YOLO（You Only Look Once）算法通过在单个网络中直接预测边界框和类别概率，采用不同的策略来分配样本。 正负样本？预测正确的框和预测失败的框 正样本： 对象中心落在网格内：对于每个真实对象（ground truth object），其边界框（bounding box）的中心落在哪个网格单元内，那个网格单元就负责预测这个对象。因此，该网格单元和与之对应的预测边界框成为“正样本” 存在性置信度：与该网格相关联的“对象存在的置信度”应该接近1 类别标签：该网格单元还需要预测该对象的类别 负样本： 对象中心不落在网格内：如果一个网格单元内没有任何真实对象（ground truth object）的中心，那么该网格单元就是一个“负样本” 存在性置信度：与这些负样本网格相关联的“对象存在的置信度”接近0 用了BN为啥偏置没用如图所示，加不加偏置都一样","categories":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}],"tags":[],"keywords":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}]},{"title":"blog书写说明","slug":"blog书写说明","date":"2022-05-10T07:58:39.000Z","updated":"2024-03-09T08:37:46.058Z","comments":true,"path":"2022/05/10/blog书写说明/","link":"","permalink":"https://heart74.github.io/2022/05/10/blog书写说明/","excerpt":"","text":"主题使用说明https://docs.hojun.cn/sakura/docs/#/home?id=%E4%BA%A4%E6%B5%81%E7%BE%A4 注释说明title: blog书写说明 // 博客标题 author: heart74 // 作者 avatar: &#39;https://cdn.jsdelivr.net/gh/heart74/cdn@1.2/img/custom/avatar.jpg&#39; // 头像 authorLink: &#39;https://heart74.github.io&#39; // 博客链接 authorAbout: I trust nobody and nobody trusts me authorDesc: I trust nobody and nobody trusts me categories: 技术 // 代表index.html菜单栏的归档，新增categories的条目在config.yaml的归档里未创建时就不会再index.html显示 comments: true // 是否可以评论 photos: &#39;https://cdn.jsdelivr.net/gh/heart74/cdn@1.2/img/wlop/8.jpg&#39; // 文章背景 date: 2022-05-10 15:58:39 tags: // 代表index.html菜单栏的清单，新增tag的条目在config.yaml的清单里未创建时就不会再index.html显示 keywords: // 好像没啥用 description: 博客书写时顶部config说明 //文章描述 菜单栏各项图标修改源图标：https://fontawesome.com/icons 下面截图链接： http://www.tsdqq.net/web/font_awesome_4.html","categories":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}],"tags":[],"keywords":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}]},{"title":"npm和hexo安装及使用","slug":"npm-hexo-install","date":"2022-03-09T07:35:14.000Z","updated":"2024-03-09T07:58:44.624Z","comments":true,"path":"2022/03/09/npm-hexo-install/","link":"","permalink":"https://heart74.github.io/2022/03/09/npm-hexo-install/","excerpt":"","text":"当前博客npm和hexo版本 npm安装：在官网找上图指定版本安装即可 hexo 安装npm查看hexo cli版本命令: npm view hexo-cli versions 安装指定版本hexo-cli: npm install -g hexo-cli@4.2.0 hexo常用命令hexo new blog_nname # 创建博客.md hexo g # 渲染并生成页面 hexo s # 本地服务预览 hexo d # 部署 hexo clean #清除缓存，若是网页正常情况下可以忽略这条命令 配置有两个配置文件 一个主题的config.yml一个站点的config.yml，和github关联操作： 关联域名通过github的ip和自己的域名绑定即可 更换主题在blog目录中的themes文件夹中查看你自己主题是什么，下载主题文件并在站点的配置文件config.yaml修改名称即可 图床目前使用github，也可以使用其他图床，工具是typora + PicGo","categories":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}],"tags":[],"keywords":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}]},{"title":"Information Visualation","slug":"InformationVisualization","date":"2021-01-09T04:08:05.000Z","updated":"2021-01-10T05:49:51.000Z","comments":true,"path":"2021/01/09/InformationVisualization/","link":"","permalink":"https://heart74.github.io/2021/01/09/InformationVisualization/","excerpt":"","text":"Information Visualization Tips: The title of the picture is a hyperlink. Click to jump to the web page of the visual chart. EchartsInformation Visualization-Echarts(English) Information Visualization-Echarts(Chinese) TableauInformation Visualization-Tableau MatplotlibInformation Visualization-Matplotlib","categories":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}],"tags":[],"keywords":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}]},{"title":"hadoop_problems_solve","slug":"hadoop","date":"2020-06-02T09:41:53.000Z","updated":"2020-06-02T09:44:13.000Z","comments":true,"path":"2020/06/02/hadoop/","link":"","permalink":"https://heart74.github.io/2020/06/02/hadoop/","excerpt":"","text":"hadoop常见疑问及解答1、4个配置文件(含注释：单机完全分布都可用)完全分布式部署建议在master上先配置好网络，搭建好单机hadoop后复制整个master虚拟机为slave1，2…n 不过不太清楚单机和伪分布式的区别呢 core-site.xml &lt;configuration&gt; &lt;!-- 这个属性用来指定namenode的hdfs协议的文件系统通信地址，可以指定一个主机+端口，也可以指定为一个namenode服务（这个服务内部可以有多台namenode实现hadoop的namenode服务 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;!--master为自己的主机名后面的master同--&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储路径 tmp 提前创建好，前面用file:表示是本地目录。hadoop在运行过程中肯定会有临时文件或缓冲之类的，必然需要一个临时目录来存放，这里就是指定这个的 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/******/hadoop/tmp&lt;/value&gt; &lt;!--/******/为根目录到自己的hadoop安装目录--&gt; &lt;/property&gt; &lt;/configuration&gt; hdfs-site.xml &lt;property&gt; &lt;!--设置hdfs副本数量：默认为3--&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;!-- namenode数据的存放地点。也就是namenode元数据存放的地方，记录了hdfs系统中文件的元数据--&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/*******/hadoop/hdfs/name&lt;/value&gt; &lt;!--/******/为根目录到自己的hadoop安装目录--&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- datanode数据的存放地点。也就是block块存放的目录了--&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/******/hadoop/hdfs/data&lt;/value&gt; &lt;!--/******/为根目录到自己的hadoop安装目录--&gt; &lt;/property&gt; mapred-site.xml &lt;!-- 通知框架MR使用YARN --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!---- 指定mr框架jobhistory的内部通讯地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt; &lt;/property&gt; &lt;!---- 指定mr框架web查看的地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt; &lt;/property&gt; yarn-site.xml &lt;!--这个文件就是配置资源管理系统yarn了，其中主要指定了一些节点资源管理器nodemanager，以及总资源管理器resourcemanager的配置。可以看到这个配置中，跟mapreduce框架是相关的。可见yarn首先是为了支持mapreduce这个模型，之后很多其他的框架都是基于mapreduce以及yarn的功能基础上开发出来的。--&gt; &lt;!--- 启用的资源调度器主类 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;!--- ResourceManager 对客户端暴露的地址。客户端通过该地址向RM提交应用程序，杀死应用程序等 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;!-- ResourceManager 对ApplicationMaster暴露的访问地址。ApplicationMaster通过该地址向RM申请资源、释放资源等 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;!-- ResourceManager 对NodeManager暴露的地址.。NodeManager通过该地址向RM汇报心跳，领取任务等 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt; &lt;!-- 对管理员暴露的访问地址。管理员通过该地址向RM发送管理命令等 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:8033&lt;/value&gt; &lt;/property&gt; &lt;!-- ResourceManager对外web ui地址。用户可通过该地址在浏览器中查看集群各类信息 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:8088&lt;/value&gt; &lt;/property&gt; &lt;!-- NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 2、hdfs常用操作# 查看帮助： hadoop fs -help &lt;cmd&gt; # 上传： hadoop fs -put &lt;linux上文件&gt; &lt;hdfs上的路径&gt; # 查看文件内容： hadoop fs -cat &lt;hdfs上的路径&gt; # 查看文件列表： hadoop fs -ls / # 下载文件： hadoop fs -get &lt;hdfs上的路径&gt; &lt;linux上文件&gt; 3、stop-yarn.sh 时NodeManager stop 异常报错如下： slave1: nodemanager did not stop gracefully after 5 seconds: killing with kill -9 查阅好多资料结论：官网未解决 4、jps命令后slave1,slave2上不出现datanode进程，stop-all.sh时也无datanode去stop（单机和伪分布式则是master里没有datanode进程）解决： 原因可能是 hdfs/data/current/VERSION 和 hdfs/name/current/VERSION 的clusterid 不同 故： 1、修改虚拟机master上：hadoop/hdfs/data/current/VERSION 文件里的clusterid 与 hadoop/hdfs/name/current/VERSION clusterid 相同 2、修改所有的slave上：hadoop/hdfs/data/current/VERSION 文件里的clusterid与master上 hadoop/hdfs/name/current/VERSION clusterid 相同 如果是单机和伪分布式hadoop只需执行第一步 5、stop-all.sh后浏览器localhost:50070的datanode界面只随机显示一个slave和datanode解决: 因为slave们是从master完全复制过来的，故hadoop/hdfs/data/current/VERSION 文件里的datanodeuuid和storageID，都相同,slave之间会产生冲突(来自网络),故： 如果有两个slave则只需修改一个slave中hadoop/hdfs/data/current/VERSION 文件里的datanodeuuid和storageID，两者都是字母数字的组合，随便修改几个数字字母为别的值就可 6、这些都是我自己遇到并解决的问题，后续应该还会持续更新敬请期待！博客首页有联系方式欢迎提问（评论功能暂未开通）","categories":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}],"tags":[],"keywords":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}]},{"title":"MarkDown语法","slug":"MarkDown语法","date":"2020-02-22T09:07:33.000Z","updated":"2020-02-22T09:15:08.000Z","comments":true,"path":"2020/02/22/MarkDown语法/","link":"","permalink":"https://heart74.github.io/2020/02/22/MarkDown语法/","excerpt":"","text":"Markdown教学1、代码块#代码块语法 ```java ```python ```shell 1.python代码 def hello: print(&#39;hello world&#39;) hello() 2.java代码 public class Test{ public void main() } 3.shell # java -jar blog start 方便起见下面的新语法我都放到代码块里了☟2、标题六个等级 # 一级标题 ## 二级标题 ### 注意&#39;#&#39;号和内容之间加空格 ###### 一直到六级标题 一级标题二级标题注意’#’号和内容之间加空格一直到六级标题3、字体# 加粗 **我要加粗** # 代码高亮显示 ==心脏== # 删除线 ~~被删除的文字~~ # 斜体 *斜体* 我要加粗 ==心脏== 被删除的文字 斜体 4、引用# 引用语法 &gt;作者：heart &gt;&gt;作者：heart &gt;&gt;&gt;作者：heart 作者：heart 作者：heart 作者：heart 5、分割线#分割线 --- 6、图片插入# 在线图片 / 本地图片 ![my_picture](5bcae8399fdb4.jpg) --图片路径 7、超链接# 超链接语法 [hello_baidu](https://www.baidu.com/) hello_baidu 8、列表# 无序列表 - 目录一 - 目录二 - 目录三 #&#39;数字&#39;+&#39;.&#39;+&#39; &#39;+内容 1. 首页 2. hello 目录一 目录二 目录三 首页 hello 9、表格# 右键点击插入表格 成绩 语文 数学 33 33","categories":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}],"tags":[{"name":"悦读","slug":"悦读","permalink":"https://heart74.github.io/tags/悦读/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://heart74.github.io/categories/技术/"}]},{"title":"小记","slug":"小记","date":"2020-02-08T05:14:00.000Z","updated":"2021-10-23T02:07:17.000Z","comments":true,"path":"2020/02/08/小记/","link":"","permalink":"https://heart74.github.io/2020/02/08/小记/","excerpt":"","text":"hi guy！为什么会有这个呢？ 偶然看见一个非常漂亮的博客，特别羡慕。于是就想自己整一个。然而假期全交给了==王者荣耀==，我太难了巧了。最近由于一只蝙蝠，上不了学，他喵的网上看ppt那些又看不进去，老想玩手机。 于是不如搞个这个玩玩，如你所见，我成功了哈哈哈哈。功夫不负有心人，找了个个人觉得很好看的主题。 用github pages，整出了这个博客。至于买自己的域名，慢慢来，不急，来日方长。 毕竟是第一次尝试，自我感觉还挺良好。 就是好多分类标签啥的还没写自己的东西，是空白的，但是还是请多多关注，该有的都会有的，从零开始，我会在这里和大家分享自己在学习，生活各个方面的快乐。一起进步！","categories":[{"name":"生活","slug":"生活","permalink":"https://heart74.github.io/categories/生活/"}],"tags":[{"name":"悦读","slug":"悦读","permalink":"https://heart74.github.io/tags/悦读/"}],"keywords":[{"name":"生活","slug":"生活","permalink":"https://heart74.github.io/categories/生活/"}]}]}